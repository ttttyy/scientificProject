{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 20.769196\n",
      "epoch 1, loss 19.971966\n",
      "epoch 1, loss 18.586689\n",
      "epoch 1, loss 17.882977\n",
      "epoch 1, loss 16.778225\n",
      "epoch 1, loss 15.822425\n",
      "epoch 1, loss 14.993820\n",
      "epoch 1, loss 14.492506\n",
      "epoch 1, loss 13.608170\n",
      "epoch 1, loss 12.839603\n",
      "epoch 1, loss 12.215231\n",
      "epoch 1, loss 11.512462\n",
      "epoch 1, loss 10.878053\n",
      "epoch 1, loss 9.843396\n",
      "epoch 1, loss 9.222528\n",
      "epoch 1, loss 9.020955\n",
      "epoch 1, loss 8.245966\n",
      "epoch 1, loss 7.777558\n",
      "epoch 1, loss 7.410923\n",
      "epoch 1, loss 7.152803\n",
      "epoch 1, loss 6.849050\n",
      "epoch 1, loss 6.295477\n",
      "epoch 1, loss 6.030652\n",
      "epoch 1, loss 5.766658\n",
      "epoch 1, loss 5.308971\n",
      "epoch 1, loss 5.156879\n",
      "epoch 1, loss 5.034059\n",
      "epoch 1, loss 4.802435\n",
      "epoch 1, loss 4.406363\n",
      "epoch 1, loss 4.167271\n",
      "epoch 1, loss 3.879490\n",
      "epoch 1, loss 3.745396\n",
      "epoch 1, loss 3.552894\n",
      "epoch 1, loss 3.302141\n",
      "epoch 1, loss 3.173536\n",
      "epoch 1, loss 2.995917\n",
      "epoch 1, loss 2.845717\n",
      "epoch 1, loss 2.613440\n",
      "epoch 1, loss 2.339000\n",
      "epoch 1, loss 2.247792\n",
      "epoch 1, loss 2.175212\n",
      "epoch 1, loss 2.073798\n",
      "epoch 1, loss 1.986544\n",
      "epoch 1, loss 1.907494\n",
      "epoch 1, loss 1.817787\n",
      "epoch 1, loss 1.739041\n",
      "epoch 1, loss 1.650891\n",
      "epoch 1, loss 1.520324\n",
      "epoch 1, loss 1.414079\n",
      "epoch 1, loss 1.283903\n",
      "epoch 1, loss 1.254009\n",
      "epoch 1, loss 1.177131\n",
      "epoch 1, loss 1.097433\n",
      "epoch 1, loss 1.037247\n",
      "epoch 1, loss 0.976590\n",
      "epoch 1, loss 0.921571\n",
      "epoch 1, loss 0.870191\n",
      "epoch 1, loss 0.814494\n",
      "epoch 1, loss 0.777769\n",
      "epoch 1, loss 0.726405\n",
      "epoch 1, loss 0.693911\n",
      "epoch 1, loss 0.639747\n",
      "epoch 1, loss 0.598846\n",
      "epoch 1, loss 0.568456\n",
      "epoch 1, loss 0.557679\n",
      "epoch 1, loss 0.534254\n",
      "epoch 1, loss 0.504245\n",
      "epoch 1, loss 0.464465\n",
      "epoch 1, loss 0.438504\n",
      "epoch 1, loss 0.418019\n",
      "epoch 1, loss 0.395839\n",
      "epoch 1, loss 0.359815\n",
      "epoch 1, loss 0.342534\n",
      "epoch 1, loss 0.315170\n",
      "epoch 1, loss 0.298664\n",
      "epoch 1, loss 0.274038\n",
      "epoch 1, loss 0.254278\n",
      "epoch 1, loss 0.243058\n",
      "epoch 1, loss 0.237073\n",
      "epoch 1, loss 0.216764\n",
      "epoch 1, loss 0.203833\n",
      "epoch 1, loss 0.185785\n",
      "epoch 1, loss 0.172744\n",
      "epoch 1, loss 0.165984\n",
      "epoch 1, loss 0.158499\n",
      "epoch 1, loss 0.144099\n",
      "epoch 1, loss 0.138497\n",
      "epoch 1, loss 0.134036\n",
      "epoch 1, loss 0.120544\n",
      "epoch 1, loss 0.115593\n",
      "epoch 1, loss 0.107262\n",
      "epoch 1, loss 0.102153\n",
      "epoch 1, loss 0.094707\n",
      "epoch 1, loss 0.088415\n",
      "epoch 1, loss 0.083167\n",
      "epoch 1, loss 0.076664\n",
      "epoch 1, loss 0.067476\n",
      "epoch 1, loss 0.065171\n",
      "epoch 1, loss 0.061457\n",
      "epoch 1, loss 0.056098\n",
      "epoch 2, loss 0.053237\n",
      "epoch 2, loss 0.049925\n",
      "epoch 2, loss 0.048113\n",
      "epoch 2, loss 0.045176\n",
      "epoch 2, loss 0.044065\n",
      "epoch 2, loss 0.042760\n",
      "epoch 2, loss 0.039271\n",
      "epoch 2, loss 0.037680\n",
      "epoch 2, loss 0.034665\n",
      "epoch 2, loss 0.031711\n",
      "epoch 2, loss 0.029294\n",
      "epoch 2, loss 0.028032\n",
      "epoch 2, loss 0.027058\n",
      "epoch 2, loss 0.025634\n",
      "epoch 2, loss 0.024510\n",
      "epoch 2, loss 0.023067\n",
      "epoch 2, loss 0.021757\n",
      "epoch 2, loss 0.020659\n",
      "epoch 2, loss 0.019750\n",
      "epoch 2, loss 0.018888\n",
      "epoch 2, loss 0.018094\n",
      "epoch 2, loss 0.017062\n",
      "epoch 2, loss 0.016587\n",
      "epoch 2, loss 0.015839\n",
      "epoch 2, loss 0.013925\n",
      "epoch 2, loss 0.013064\n",
      "epoch 2, loss 0.012502\n",
      "epoch 2, loss 0.012327\n",
      "epoch 2, loss 0.011947\n",
      "epoch 2, loss 0.011123\n",
      "epoch 2, loss 0.010788\n",
      "epoch 2, loss 0.009955\n",
      "epoch 2, loss 0.008962\n",
      "epoch 2, loss 0.008499\n",
      "epoch 2, loss 0.007894\n",
      "epoch 2, loss 0.007673\n",
      "epoch 2, loss 0.007426\n",
      "epoch 2, loss 0.006950\n",
      "epoch 2, loss 0.006615\n",
      "epoch 2, loss 0.006176\n",
      "epoch 2, loss 0.006028\n",
      "epoch 2, loss 0.005528\n",
      "epoch 2, loss 0.005078\n",
      "epoch 2, loss 0.004732\n",
      "epoch 2, loss 0.004313\n",
      "epoch 2, loss 0.004042\n",
      "epoch 2, loss 0.003825\n",
      "epoch 2, loss 0.003582\n",
      "epoch 2, loss 0.003417\n",
      "epoch 2, loss 0.003322\n",
      "epoch 2, loss 0.003205\n",
      "epoch 2, loss 0.003113\n",
      "epoch 2, loss 0.003078\n",
      "epoch 2, loss 0.002923\n",
      "epoch 2, loss 0.002771\n",
      "epoch 2, loss 0.002671\n",
      "epoch 2, loss 0.002603\n",
      "epoch 2, loss 0.002476\n",
      "epoch 2, loss 0.002310\n",
      "epoch 2, loss 0.002163\n",
      "epoch 2, loss 0.001968\n",
      "epoch 2, loss 0.001756\n",
      "epoch 2, loss 0.001693\n",
      "epoch 2, loss 0.001584\n",
      "epoch 2, loss 0.001505\n",
      "epoch 2, loss 0.001405\n",
      "epoch 2, loss 0.001320\n",
      "epoch 2, loss 0.001261\n",
      "epoch 2, loss 0.001157\n",
      "epoch 2, loss 0.001093\n",
      "epoch 2, loss 0.001015\n",
      "epoch 2, loss 0.000935\n",
      "epoch 2, loss 0.000861\n",
      "epoch 2, loss 0.000830\n",
      "epoch 2, loss 0.000810\n",
      "epoch 2, loss 0.000783\n",
      "epoch 2, loss 0.000741\n",
      "epoch 2, loss 0.000712\n",
      "epoch 2, loss 0.000695\n",
      "epoch 2, loss 0.000663\n",
      "epoch 2, loss 0.000625\n",
      "epoch 2, loss 0.000582\n",
      "epoch 2, loss 0.000490\n",
      "epoch 2, loss 0.000460\n",
      "epoch 2, loss 0.000438\n",
      "epoch 2, loss 0.000412\n",
      "epoch 2, loss 0.000394\n",
      "epoch 2, loss 0.000377\n",
      "epoch 2, loss 0.000359\n",
      "epoch 2, loss 0.000342\n",
      "epoch 2, loss 0.000320\n",
      "epoch 2, loss 0.000303\n",
      "epoch 2, loss 0.000287\n",
      "epoch 2, loss 0.000266\n",
      "epoch 2, loss 0.000255\n",
      "epoch 2, loss 0.000245\n",
      "epoch 2, loss 0.000233\n",
      "epoch 2, loss 0.000213\n",
      "epoch 2, loss 0.000203\n",
      "epoch 2, loss 0.000198\n",
      "epoch 3, loss 0.000191\n",
      "epoch 3, loss 0.000188\n",
      "epoch 3, loss 0.000179\n",
      "epoch 3, loss 0.000170\n",
      "epoch 3, loss 0.000164\n",
      "epoch 3, loss 0.000157\n",
      "epoch 3, loss 0.000151\n",
      "epoch 3, loss 0.000144\n",
      "epoch 3, loss 0.000139\n",
      "epoch 3, loss 0.000132\n",
      "epoch 3, loss 0.000121\n",
      "epoch 3, loss 0.000114\n",
      "epoch 3, loss 0.000113\n",
      "epoch 3, loss 0.000110\n",
      "epoch 3, loss 0.000107\n",
      "epoch 3, loss 0.000106\n",
      "epoch 3, loss 0.000102\n",
      "epoch 3, loss 0.000098\n",
      "epoch 3, loss 0.000094\n",
      "epoch 3, loss 0.000092\n",
      "epoch 3, loss 0.000090\n",
      "epoch 3, loss 0.000089\n",
      "epoch 3, loss 0.000088\n",
      "epoch 3, loss 0.000087\n",
      "epoch 3, loss 0.000085\n",
      "epoch 3, loss 0.000082\n",
      "epoch 3, loss 0.000080\n",
      "epoch 3, loss 0.000080\n",
      "epoch 3, loss 0.000078\n",
      "epoch 3, loss 0.000076\n",
      "epoch 3, loss 0.000074\n",
      "epoch 3, loss 0.000073\n",
      "epoch 3, loss 0.000071\n",
      "epoch 3, loss 0.000069\n",
      "epoch 3, loss 0.000068\n",
      "epoch 3, loss 0.000067\n",
      "epoch 3, loss 0.000066\n",
      "epoch 3, loss 0.000065\n",
      "epoch 3, loss 0.000064\n",
      "epoch 3, loss 0.000063\n",
      "epoch 3, loss 0.000063\n",
      "epoch 3, loss 0.000063\n",
      "epoch 3, loss 0.000061\n",
      "epoch 3, loss 0.000060\n",
      "epoch 3, loss 0.000060\n",
      "epoch 3, loss 0.000059\n",
      "epoch 3, loss 0.000058\n",
      "epoch 3, loss 0.000057\n",
      "epoch 3, loss 0.000057\n",
      "epoch 3, loss 0.000056\n",
      "epoch 3, loss 0.000056\n",
      "epoch 3, loss 0.000055\n",
      "epoch 3, loss 0.000055\n",
      "epoch 3, loss 0.000055\n",
      "epoch 3, loss 0.000055\n",
      "epoch 3, loss 0.000055\n",
      "epoch 3, loss 0.000054\n",
      "epoch 3, loss 0.000054\n",
      "epoch 3, loss 0.000054\n",
      "epoch 3, loss 0.000053\n",
      "epoch 3, loss 0.000053\n",
      "epoch 3, loss 0.000053\n",
      "epoch 3, loss 0.000053\n",
      "epoch 3, loss 0.000053\n",
      "epoch 3, loss 0.000053\n",
      "epoch 3, loss 0.000053\n",
      "epoch 3, loss 0.000053\n",
      "epoch 3, loss 0.000052\n",
      "epoch 3, loss 0.000052\n",
      "epoch 3, loss 0.000052\n",
      "epoch 3, loss 0.000051\n",
      "epoch 3, loss 0.000051\n",
      "epoch 3, loss 0.000051\n",
      "epoch 3, loss 0.000051\n",
      "epoch 3, loss 0.000051\n",
      "epoch 3, loss 0.000050\n",
      "epoch 3, loss 0.000050\n",
      "epoch 3, loss 0.000051\n",
      "epoch 3, loss 0.000051\n",
      "epoch 3, loss 0.000051\n",
      "epoch 3, loss 0.000050\n",
      "epoch 3, loss 0.000050\n",
      "epoch 3, loss 0.000050\n",
      "epoch 3, loss 0.000050\n",
      "epoch 3, loss 0.000050\n",
      "epoch 3, loss 0.000050\n",
      "epoch 3, loss 0.000050\n",
      "epoch 3, loss 0.000050\n",
      "epoch 3, loss 0.000050\n",
      "epoch 3, loss 0.000050\n",
      "epoch 3, loss 0.000050\n",
      "epoch 3, loss 0.000050\n",
      "epoch 3, loss 0.000049\n",
      "epoch 3, loss 0.000049\n",
      "epoch 3, loss 0.000049\n",
      "epoch 3, loss 0.000049\n",
      "epoch 3, loss 0.000049\n",
      "epoch 3, loss 0.000050\n",
      "epoch 3, loss 0.000049\n",
      "epoch 3, loss 0.000049\n",
      "w的估计误差: tensor([ 0.0007, -0.0003], grad_fn=<SubBackward0>)\n",
      "b的估计误差: tensor([0.0007], grad_fn=<RsubBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "def synthetic_data(w,b,num_examples):\n",
    "    X = torch.normal(0,1,(num_examples,len(w)))\n",
    "    y = torch.matmul(X,w) + b\n",
    "    y += torch.normal(0,0.01,y.shape)\n",
    "    return X,y.reshape((-1,1))\n",
    "true_w = torch.tensor([2,-3.5])\n",
    "true_b = 5.3\n",
    "features,labels = synthetic_data(true_w,true_b,1000)\n",
    "def data_iter(batch_size,features,labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0,num_examples,batch_size):\n",
    "        batch_indices = torch.tensor(\n",
    "            indices[i:min(i+batch_size,num_examples)]\n",
    "        )\n",
    "        yield features[batch_indices],labels[batch_indices]\n",
    "w = torch.normal(0,0.01,size=(2,1),requires_grad=True)\n",
    "b = torch.zeros(1,requires_grad=True)\n",
    "\n",
    "def linreg(X,w,b):\n",
    "    return torch.matmul(X,w) + b\n",
    "\n",
    "def squared_loss(y_hat, y):\n",
    "    \"\"\"均方损失\"\"\"\n",
    "    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2\n",
    "def sgd(params, lr, batch_size):  #@save\n",
    "    \"\"\"小批量随机梯度下降\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size\n",
    "            param.grad.zero_()\n",
    "lr = 0.03\n",
    "num_epochs = 3\n",
    "batch_size = 10\n",
    "net = linreg\n",
    "loss = squared_loss\n",
    "for epoch in range(num_epochs):\n",
    "    for X,y in data_iter(10,features,labels):\n",
    "        l = loss(net(X,w,b),y)\n",
    "        l.sum().backward()\n",
    "        sgd([w,b],lr,batch_size)\n",
    "        with torch.no_grad():\n",
    "            train_l = loss(net(features, w, b), labels)\n",
    "            print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')\n",
    "print(f'w的估计误差: {true_w - w.reshape(true_w.shape)}')\n",
    "print(f'b的估计误差: {true_b - b}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
