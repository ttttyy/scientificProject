{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 6, 28, 28]             156\n",
      "            Conv2d-2           [-1, 16, 10, 10]           2,416\n",
      "            Linear-3                  [-1, 120]          48,120\n",
      "            Linear-4                   [-1, 84]          10,164\n",
      "            Linear-5                   [-1, 10]             850\n",
      "================================================================\n",
      "Total params: 61,706\n",
      "Trainable params: 61,706\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.05\n",
      "Params size (MB): 0.24\n",
      "Estimated Total Size (MB): 0.29\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (256x46656 and 400x120)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [1], line 58\u001B[0m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m idx,(train_x,train_label) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(data_loader_train):\n\u001B[0;32m     57\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 58\u001B[0m     pred_y \u001B[38;5;241m=\u001B[39m \u001B[43mnet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_x\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     59\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss_fn(pred_y,train_label\u001B[38;5;241m.\u001B[39mlong())\n\u001B[0;32m     60\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m idx \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m10000\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[1;32mD:\\miniconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[1;32mIn [1], line 43\u001B[0m, in \u001B[0;36mNet.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     41\u001B[0m x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mmax_pool2d(F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv2(x)),\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m     42\u001B[0m x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mflatten(x,\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m---> 43\u001B[0m x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfc1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     44\u001B[0m x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc2(x))\n\u001B[0;32m     45\u001B[0m x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc3(x))\n",
      "File \u001B[1;32mD:\\miniconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mD:\\miniconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (256x46656 and 400x120)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "from torchvision import datasets,transforms\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize((224,224)),\n",
    "                                transforms.ToTensor(),\n",
    "                               transforms.Normalize(mean=[0.5,],std=[0.5,])])\n",
    "\n",
    "data_train = datasets.FashionMNIST(root=\"./data\",\n",
    "                                   transform=transform,\n",
    "                                   train=True,\n",
    "                                   download=True)\n",
    "data_test = datasets.FashionMNIST(\n",
    "    root=\"./data\",\n",
    "    transform=transform,\n",
    "    train=False\n",
    ")\n",
    "\n",
    "data_loader_train = torch.utils.data.DataLoader(dataset=data_train,\n",
    "                                                batch_size=256,\n",
    "                                                shuffle=True)\n",
    "data_loader_test = torch.utils.data.DataLoader(dataset=data_test,\n",
    "                                               batch_size=256,\n",
    "                                               shuffle=True)\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,6,5,padding=2)\n",
    "        self.conv2 = nn.Conv2d(6,16,5)\n",
    "        self.fc1 = nn.Linear(16*5*5,120)\n",
    "        self.fc2 = nn.Linear(120,84)\n",
    "        self.fc3 = nn.Linear(84,10)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)),2,2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)),2)\n",
    "        x = torch.flatten(x,1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "lr = 0.5\n",
    "num_epochs = 10\n",
    "net = Net()\n",
    "summary(net,(1,28,28))\n",
    "optimizer = torch.optim.SGD(net.parameters(),lr = lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    for idx,(train_x,train_label) in enumerate(data_loader_train):\n",
    "        optimizer.zero_grad()\n",
    "        pred_y = net(train_x)\n",
    "        loss = loss_fn(pred_y,train_label.long())\n",
    "        if idx % 10000 == 0:\n",
    "            print(\"loss:{:.4f}\".format(loss))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    all_correct_num = 0\n",
    "    all_sample_num = 0\n",
    "    net.eval()\n",
    "    for i, (test_x,test_label) in enumerate(data_loader_test):\n",
    "        pred_y = net(test_x.float()).detach()\n",
    "        pred_y = np.argmax(pred_y,axis=-1)\n",
    "        current_correct_num = pred_y == test_label\n",
    "        all_correct_num += np.sum(current_correct_num.numpy(),axis=-1)\n",
    "        all_sample_num += current_correct_num.shape[0]\n",
    "    acc = all_correct_num / all_sample_num\n",
    "    print(\"epoch {:d}: acc = {:.4f}\".format(epoch,acc))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
